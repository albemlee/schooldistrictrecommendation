{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Working Directory\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Requirements\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for classificaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "wrangled_data = pd.read_csv('output/wrangled_data_ii.csv', dtype='str', na_values='NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_columns_classification(wrangled_data_ii, feature_set=None):\n",
    "    \"\"\"\n",
    "    Return X_train, X_dev, X_test, y_train, y_dev, y_test: predictors (X) and label (y) of train, dev, and test sets (0.8, 0.1, 0.1 split)\n",
    "\n",
    "    param dataframe wrangled_data_ii: dataframe of wrangled dataframe (after rename/reduce columns)\n",
    "    param list feature_set: list of features to include\n",
    "    \"\"\"\n",
    "\n",
    "    prepped_classification_data = wrangled_data_ii.copy()\n",
    "\n",
    "    # Specify features to include in model\n",
    "    if feature_set:\n",
    "        pass\n",
    "    else:\n",
    "        feature_set = wrangled_data_ii.drop(columns=['NAME', 'LEAID', 'exist_five_years']).columns\n",
    "    X = wrangled_data_ii[feature_set].copy()\n",
    "\n",
    "    # Identify column types\n",
    "    identifying_columns = ['NAME', 'LEAID']\n",
    "    prediction_columns = ['exist_five_years']\n",
    "    categorical_columns = ['lowest_grade', 'highest_grade', 'charter_status']\n",
    "    boolean_columns = ['bureau_indian_education']\n",
    "    numerical_columns = []\n",
    "\n",
    "    # identify numerical columns\n",
    "    for column in X.columns:\n",
    "        if column in identifying_columns or column in categorical_columns or column in boolean_columns or column in prediction_columns:\n",
    "            pass\n",
    "        elif len(X[column].unique()) > 100:\n",
    "            numerical_columns.append(column)\n",
    "        else:\n",
    "            categorical_columns.append(column)\n",
    "\n",
    "    X[numerical_columns] = X[numerical_columns].astype(float)\n",
    "\n",
    "    # one hot encode categorical variables\n",
    "    X = pd.get_dummies(X, prefix_sep='_', columns=categorical_columns, drop_first=True)\n",
    "\n",
    "    # Split into train and test sets\n",
    "    y = wrangled_data_ii[prediction_columns].apply(lambda x: x=='False')\n",
    "    y = y.values.ravel()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)\n",
    "    X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=21)\n",
    "\n",
    "    return X_train, y_train, X_dev, y_dev, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_dev, y_dev, X_test, y_test = prep_columns_classification(wrangled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06011372867587327"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.concatenate([y_train, y_dev, y_test])\n",
    "sum(y)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(clf_pipeline, X_train, y_train, X_dev, y_dev):\n",
    "    \"\"\"\n",
    "    Return sklearn pipeline object clf_pipeline: trained sklearn pipeline\n",
    "    Return recall_train, recall_dev, precision_train, precision_dev: recall and precision of training and development sets\n",
    "    \n",
    "    param sklearn pipeline object clf_pipeline: untrained sklearn pipeline\n",
    "    param np.array X_train, y_train, X_dev, y_dev: feature (X) and labels (y) of training and development sets\n",
    "    \"\"\"\n",
    "    \n",
    "    warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "    \n",
    "    # Fit pipeline using training data\n",
    "    clf_pipeline.fit(X_train, y_train) \n",
    "\n",
    "    # Get predictions for training and development sets\n",
    "    train_predictions = clf_pipeline.predict(X_train)\n",
    "    dev_predictions = clf_pipeline.predict(X_dev)\n",
    "    \n",
    "    # Calculate recall and precision of training and development sets\n",
    "    recall_train = recall_score(y_train, train_predictions)\n",
    "    recall_dev = recall_score(y_dev, dev_predictions)\n",
    "    precision_train = precision_score(y_train, train_predictions)\n",
    "    precision_dev = precision_score(y_dev, dev_predictions)\n",
    "\n",
    "    return clf_pipeline, recall_train, recall_dev, precision_train, precision_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on training data:       0.018\n",
      "Recall on development data:    0.043478260869565216\n",
      "Precision on training data:    0.2571428571428571\n",
      "Precision on development data: 0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "wrangled_data_woNA = wrangled_data.dropna().copy()\n",
    "X_train_woNA, y_train_woNA, X_dev_woNA, y_dev_woNA, X_test_woNA, y_test_woNA = prep_columns_classification(wrangled_data_woNA)\n",
    "\n",
    "baseline_pipeline = Pipeline([ \n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "baseline_pipeline, recall_train, recall_dev, precision_train, precision_dev = test_model(baseline_pipeline, X_train_woNA, y_train_woNA, X_dev_woNA, y_dev_woNA)\n",
    "\n",
    "print(\"Recall on training data:       {}\".format(recall_train))\n",
    "print(\"Recall on development data:    {}\".format(recall_dev))\n",
    "print(\"Precision on training data:    {}\".format(precision_train))\n",
    "print(\"Precision on development data: {}\".format(precision_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on training data:       0.134\n",
      "Recall on development data:    0.11594202898550725\n",
      "Precision on training data:    0.9710144927536232\n",
      "Precision on development data: 1.0\n"
     ]
    }
   ],
   "source": [
    "minmax_pipeline = Pipeline([ \n",
    "    ('scaling', MinMaxScaler()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "minmax_pipeline, recall_train, recall_dev, precision_train, precision_dev = test_model(minmax_pipeline, X_train_woNA, y_train_woNA, X_dev_woNA, y_dev_woNA)\n",
    "\n",
    "print(\"Recall on training data:       {}\".format(recall_train))\n",
    "print(\"Recall on development data:    {}\".format(recall_dev))\n",
    "print(\"Precision on training data:    {}\".format(precision_train))\n",
    "print(\"Precision on development data: {}\".format(precision_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on training data:       0.808\n",
      "Recall on development data:    0.8405797101449275\n",
      "Precision on training data:    0.1805183199285076\n",
      "Precision on development data: 0.22137404580152673\n"
     ]
    }
   ],
   "source": [
    "weighting_pipeline = Pipeline([ \n",
    "    ('scaling', MinMaxScaler()),\n",
    "    ('clf', LogisticRegression(class_weight='balanced'))\n",
    "])\n",
    "weighting_pipeline, recall_train, recall_dev, precision_train, precision_dev = test_model(weighting_pipeline, X_train_woNA, y_train_woNA, X_dev_woNA, y_dev_woNA)\n",
    "\n",
    "print(\"Recall on training data:       {}\".format(recall_train))\n",
    "print(\"Recall on development data:    {}\".format(recall_dev))\n",
    "print(\"Precision on training data:    {}\".format(precision_train))\n",
    "print(\"Precision on development data: {}\".format(precision_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on training data:       0.936\n",
      "Recall on development data:    0.7681159420289855\n",
      "Precision on training data:    0.2881773399014778\n",
      "Precision on development data: 0.2849462365591398\n"
     ]
    }
   ],
   "source": [
    "XGB_pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('clf', XGBClassifier(scale_pos_weight=(len(y_train)-sum(y_train))/sum(y_train)))\n",
    "])\n",
    "XGB_pipeline, recall_train, recall_dev, precision_train, precision_dev = test_model(XGB_pipeline, X_train_woNA, y_train_woNA, X_dev_woNA, y_dev_woNA)\n",
    "\n",
    "print(\"Recall on training data:       {}\".format(recall_train))\n",
    "print(\"Recall on development data:    {}\".format(recall_dev))\n",
    "print(\"Precision on training data:    {}\".format(precision_train))\n",
    "print(\"Precision on development data: {}\".format(precision_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on training data:       0.872\n",
      "Recall on development data:    0.6811594202898551\n",
      "Precision on training data:    0.21920563097033685\n",
      "Precision on development data: 0.1821705426356589\n"
     ]
    }
   ],
   "source": [
    "PCA_pipeline = Pipeline([\n",
    "    ('pca', PCA(n_components=20)),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('clf', XGBClassifier(scale_pos_weight=(len(y_train)-sum(y_train))/sum(y_train)))\n",
    "])\n",
    "PCA_pipeline, recall_train, recall_dev, precision_train, precision_dev = test_model(PCA_pipeline, X_train_woNA, y_train_woNA, X_dev_woNA, y_dev_woNA)\n",
    "\n",
    "print(\"Recall on training data:       {}\".format(recall_train))\n",
    "print(\"Recall on development data:    {}\".format(recall_dev))\n",
    "print(\"Precision on training data:    {}\".format(precision_train))\n",
    "print(\"Precision on development data: {}\".format(precision_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on training data:       0.9395667046750285\n",
      "Recall on development data:    0.8\n",
      "Precision on training data:    0.28084526244035446\n",
      "Precision on development data: 0.2413793103448276\n"
     ]
    }
   ],
   "source": [
    "impute_pipeline = Pipeline([\n",
    "    ('imp', Imputer(missing_values='NaN', strategy='most_frequent', axis=0)),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('clf', XGBClassifier(scale_pos_weight=(len(y_train)-sum(y_train))/sum(y_train)))\n",
    "])\n",
    "impute_pipeline, recall_train, recall_dev, precision_train, precision_dev = test_model(impute_pipeline, X_train, y_train, X_dev, y_dev)\n",
    "\n",
    "print(\"Recall on training data:       {}\".format(recall_train))\n",
    "print(\"Recall on development data:    {}\".format(recall_dev))\n",
    "print(\"Precision on training data:    {}\".format(precision_train))\n",
    "print(\"Precision on development data: {}\".format(precision_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on training data:       0.9179019384264538\n",
      "Recall on development data:    0.8095238095238095\n",
      "Precision on training data:    0.28505665722379603\n",
      "Precision on development data: 0.23943661971830985\n"
     ]
    }
   ],
   "source": [
    "impute_pipeline = Pipeline([\n",
    "    ('imp', Imputer(missing_values='NaN', strategy='mean', axis=0)),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('clf', XGBClassifier(scale_pos_weight=(len(y_train)-sum(y_train))/sum(y_train)))\n",
    "])\n",
    "impute_pipeline, recall_train, recall_dev, precision_train, precision_dev = test_model(impute_pipeline, X_train, y_train, X_dev, y_dev)\n",
    "\n",
    "print(\"Recall on training data:       {}\".format(recall_train))\n",
    "print(\"Recall on development data:    {}\".format(recall_dev))\n",
    "print(\"Precision on training data:    {}\".format(precision_train))\n",
    "print(\"Precision on development data: {}\".format(precision_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on training data:       0.9372862029646523\n",
      "Recall on development data:    0.8095238095238095\n",
      "Precision on training data:    0.276674520363514\n",
      "Precision on development data: 0.23480662983425415\n"
     ]
    }
   ],
   "source": [
    "impute_pipeline = Pipeline([\n",
    "    ('imp', Imputer(missing_values='NaN', strategy='median', axis=0)),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('clf', XGBClassifier(scale_pos_weight=(len(y_train)-sum(y_train))/sum(y_train)))\n",
    "])\n",
    "impute_pipeline, recall_train, recall_dev, precision_train, precision_dev = test_model(impute_pipeline, X_train, y_train, X_dev, y_dev)\n",
    "\n",
    "print(\"Recall on training data:       {}\".format(recall_train))\n",
    "print(\"Recall on development data:    {}\".format(recall_dev))\n",
    "print(\"Precision on training data:    {}\".format(precision_train))\n",
    "print(\"Precision on development data: {}\".format(precision_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_10 = ['total_local_revenue', \n",
    "               'total_state_revenue', \n",
    "               'total_federal_revenue',\n",
    "               'teachers_total', \n",
    "               'charter_status', \n",
    "               'white_students', \n",
    "               'total_schools',\n",
    "               'total_students',\n",
    "               'lowest_grade', \n",
    "               'highest_grade']\n",
    "\n",
    "X_train, y_train, X_dev, y_dev, X_test, y_test = prep_columns_classification(wrangled_data, features_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on training data:       0.8620296465222349\n",
      "Recall on development data:    0.7904761904761904\n",
      "Precision on training data:    0.2644281217208814\n",
      "Precision on development data: 0.2292817679558011\n"
     ]
    }
   ],
   "source": [
    "tenfeatures_pipeline = Pipeline([\n",
    "    ('imp', Imputer(missing_values='NaN', strategy='median', axis=0)),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('clf', XGBClassifier(scale_pos_weight=(len(y_train)-sum(y_train))/sum(y_train)))\n",
    "])\n",
    "tenfeatures_pipeline, recall_train, recall_dev, precision_train, precision_dev = test_model(tenfeatures_pipeline, X_train, y_train, X_dev, y_dev)\n",
    "\n",
    "print(\"Recall on training data:       {}\".format(recall_train))\n",
    "print(\"Recall on development data:    {}\".format(recall_dev))\n",
    "print(\"Precision on training data:    {}\".format(precision_train))\n",
    "print(\"Precision on development data: {}\".format(precision_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_15 = ['total_local_revenue', \n",
    "               'total_state_revenue', \n",
    "               'total_federal_revenue',\n",
    "               'teachers_total', \n",
    "               'charter_status', \n",
    "               'white_students', \n",
    "               'total_schools',\n",
    "               'total_students',\n",
    "               'lowest_grade', \n",
    "               'highest_grade', \n",
    "               'state_name', \n",
    "               'total_expenditure', \n",
    "               'administrators_school', \n",
    "               'metro_micro', \n",
    "               'white_male_students']\n",
    "\n",
    "X_train, y_train, X_dev, y_dev, X_test, y_test = prep_columns_classification(wrangled_data, features_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on training data:       0.8962371721778791\n",
      "Recall on development data:    0.8476190476190476\n",
      "Precision on training data:    0.2661699966136133\n",
      "Precision on development data: 0.23177083333333334\n"
     ]
    }
   ],
   "source": [
    "fifteenfeatures_pipeline = Pipeline([\n",
    "    ('imp', Imputer(missing_values='NaN', strategy='median', axis=0)),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('clf', XGBClassifier(scale_pos_weight=(len(y_train)-sum(y_train))/sum(y_train)))\n",
    "])\n",
    "fifteenfeatures_pipeline, recall_train, recall_dev, precision_train, precision_dev = test_model(fifteenfeatures_pipeline, X_train, y_train, X_dev, y_dev)\n",
    "\n",
    "print(\"Recall on training data:       {}\".format(recall_train))\n",
    "print(\"Recall on development data:    {}\".format(recall_dev))\n",
    "print(\"Precision on training data:    {}\".format(precision_train))\n",
    "print(\"Precision on development data: {}\".format(precision_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/pipelines/classify_pipeline.sav']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'output/pipelines/classify_pipeline.sav'\n",
    "joblib.dump(fifteenfeatures_pipeline, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_train, X_dev, X_test])\n",
    "y = np.concatenate([y_train, y_dev, y_test])\n",
    "predictions = fifteenfeatures_pipeline.predict(X)\n",
    "classification_frame = pd.DataFrame({'LEAID': wrangled_data.loc[X.index, 'LEAID'], \n",
    "                                     'NAME': wrangled_data.loc[X.index, 'NAME'], \n",
    "                                     'Close_Five_Years_Actual': y, \n",
    "                                     'Close_Five_Years_Prediction': predictions})\n",
    "classification_frame.to_csv('output/classification_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Features from EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "wrangled_data = pd.read_csv('output/wrangled_data_ii.csv', dtype='str', na_values='NaN')\n",
    "\n",
    "keep_features = ['total_students',\n",
    "                 'total_schools',\n",
    "                 'teachers_total',\n",
    "                 'total_revenue',\n",
    "                 'total_federal_revenue',\n",
    "                 'total_state_revenue',\n",
    "                 'total_local_revenue',\n",
    "                 'total_expenditure',\n",
    "                 'total_salaries',\n",
    "                 'white_students',\n",
    "                 'lowest_grade',\n",
    "                 'highest_grade',\n",
    "                 'metro_micro',\n",
    "                 'charter_status',\n",
    "                 'state_name']\n",
    "\n",
    "\n",
    "X_train, y_train, X_dev, y_dev, X_test, y_test = prep_columns_classification(wrangled_data, keep_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on training data:       0.8962371721778791\n",
      "Recall on development data:    0.8571428571428571\n",
      "Precision on training data:    0.26061007957559684\n",
      "Precision on development data: 0.2349869451697128\n"
     ]
    }
   ],
   "source": [
    "clf_pipeline = Pipeline([\n",
    "    ('imp', Imputer(missing_values='NaN', strategy='median', axis=0)),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('clf', XGBClassifier(scale_pos_weight=(len(y_train)-sum(y_train))/sum(y_train)))\n",
    "])\n",
    "clf_pipeline, recall_train, recall_dev, precision_train, precision_dev = test_model(clf_pipeline, X_train, y_train, X_dev, y_dev)\n",
    "\n",
    "print(\"Recall on training data:       {}\".format(recall_train))\n",
    "print(\"Recall on development data:    {}\".format(recall_dev))\n",
    "print(\"Precision on training data:    {}\".format(precision_train))\n",
    "print(\"Precision on development data: {}\".format(precision_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/pipelines/classify_pipeline.sav']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'output/pipelines/classify_pipeline.sav'\n",
    "joblib.dump(clf_pipeline, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_train, X_dev, X_test])\n",
    "y = np.concatenate([y_train, y_dev, y_test])\n",
    "predictions = clf_pipeline.predict(X)\n",
    "classification_frame = pd.DataFrame({'LEAID': wrangled_data.loc[X.index, 'LEAID'], \n",
    "                                     'NAME': wrangled_data.loc[X.index, 'NAME'], \n",
    "                                     'Close_Five_Years_Actual': y, \n",
    "                                     'Close_Five_Years_Prediction': predictions})\n",
    "classification_frame.to_csv('output/classification_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
